Get arxiv metadata, embedding by specter2base.

Here is the updated README content, incorporating the paperdb data pipeline and the requested citations. You can copy this directly.

Community-Federated Conference (CFC) Curation Engine & XtraMCP Researcher Node

This repository houses the core intelligence for semantic literature retrieval and topic discovery. It serves as the backend engine for the Researcher Module within the XtraMCP framework [1], powering the PaperDebugger writing assistant [3]. Furthermore, it provides the technological infrastructure required to operationalize the Community-Federated Conference (CFC) model, addressing the sustainability crisis in centralized AI conferences [2].

1. Data Ingestion: The paperdb Pipeline

The foundation of this system lies in the paperdb/ directory, which constructs the comprehensive knowledge base required for semantic analysis.

Function: This module scrapes and aggregates arXiv metadata and utilizes Specter2 to generate document-level vector embeddings.

Outputs: It produces two critical artifacts:

arxiv_merged.parquet: A structured dataset containing metadata (titles, abstracts, authors, categories) for ~700,000 papers.

embeddings.npy: A dense vector file containing the pre-computed embeddings.

Integration: These files are loaded directly by app.py to initialize the WorkshopRecommender class, enabling low-latency retrieval without the need for real-time inference on the entire corpus.

2. Core Functionalities: Context2Papers & Papers2Context

The app.py engine utilizes the data from paperdb to implement two bidirectional flows of information:

Context2Papers: Semantic Retrieval & Recommendation

Goal: To retrieve high-precision, relevant academic papers given a specific research context (e.g., a workshop description, abstract, or research query).

Implementation:

Vectorization (Recall): We use Specter2 (allenai/specter2_base) to convert user queries into vectors compatible with the pre-computed embeddings.npy. This allows for dense retrieval via Cosine Similarity to recall the top-k candidates1111.

Re-ranking (Precision): To correct for information loss in vector compression, we employ a Large Language Model Re-ranker (Qwen/Qwen3-Reranker-8B). This re-scores candidates by analyzing the full contextual alignment between the query and the paper, ensuring superior relevance222222222.

Papers2Context: Topic Discovery & Trend Analysis

Goal: To automatically discover latent themes, research trends, and "contexts" from the massive collection of papers in arxiv_merged.parquet, filtered by region or time.

Implementation:

Clustering Pipeline: We implement BERTopic to perform dynamic topic modeling.

Dimensionality Reduction: UMAP is used to reduce the high-dimensional Specter2 embeddings into a clusterable space.

Density Clustering: HDBSCAN identifies dense clusters of papers representing emerging research fronts.

Topic Representation: CountVectorizer extracts class-based TF-IDF keywords to generate human-readable labels for each identified context.

3. Role in XtraMCP & PaperDebugger

This repository functions as the Researcher Node within the XtraMCP architecture, decoupling orchestration from reasoning [1].

The Researcher of XtraMCP:

It acts as the execution layer for the Researcher Module. When the XtraMCP control layer receives a user intent (e.g., "Find related work"), it routes the request to this backend via the Model Context Protocol (MCP)3333.

It provides a Hallucination-Free Safeguard: Unlike standard LLMs that may fabricate citations, this module performs deterministic retrieval over the vector database created by paperdb, ensuring every recommendation is empirically verifiable4444.

Service to PaperDebugger:

PaperDebugger [3], the user-facing Overleaf extension, relies on this backend to provide real-time "Deep Research" capabilities.

By analyzing the user's current manuscript draft, this engine retrieves relevant literature and generates "Relevance Insights," helping authors position their work against the state-of-the-art5555.

4. Addressing the AI Conference Crisis (CFC Model)

The current centralized AI conference model is unsustainable due to exponential submission growth and inefficient knowledge dissemination [2]. This codebase serves as the technical foundation for the proposed Community-Federated Conference (CFC) solution.

Solving Information Overload:

By utilizing Context2Papers, we enable the "Scientific Mission" of efficient knowledge exchange. Researchers can cut through the noise of thousands of submissions to find work specifically relevant to their "context"6.

Enabling Federated Regional Hubs:

The CFC model proposes separating peer review from presentation via Federated Regional Hubs (local gatherings of 500-1500 participants)7.

Papers2Context is critical for organizers of these hubs. By filtering arxiv_merged.parquet by region (e.g., "Singapore") and running topic discovery, organizers can identify dominant local research themes to curate targeted, high-value workshops. This restores the "Community Building" pillar by fostering meaningful, localized interactions rather than anonymous mega-conferences8888.

References

[1] XtraMCP: A Context-Aware Assistant for Enhancing Academic Writing and Revision.

[2] Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conferences.

[3] PaperDebugger: AI-powered academic writing assistant for LaTeX & Overlea

arxiv_merged.parquet and embeddings.npy according to paperdb.
